{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a06b97-fd29-4fba-a3fe-28d2c33b9767",
   "metadata": {},
   "source": [
    "# Chapter 13 - SageMaker JumpStart: Fine-tuning Llama 3 for Summarization with SageMaker JumpStart\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to fine-tune Meta's Llama 3 8B model for text summarization tasks using Amazon SageMaker JumpStart. We'll leverage the Databricks Dolly 15k dataset to enhance the model's summarization capabilities, deploy it as a SageMaker endpoint, and compare the performance between the original and fine-tuned models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501f6a5-d045-4aff-979b-5e5f59b5a1b0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to fine-tune Meta's Llama 3 8B model for text summarization tasks using Amazon SageMaker JumpStart. We'll leverage the Databricks Dolly 15k dataset to enhance the model's summarization capabilities, deploy it as a SageMaker endpoint, and compare the performance between the original and fine-tuned models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099ad3e-8005-4efe-bf96-f6a0879132f4",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- AWS account with SageMaker access\n",
    "- Appropriate permissions for JumpStart models\n",
    "- SageMaker Execution role with S3 access\n",
    "- G5 instance quota in your AWS account\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e53388-2d27-4a43-820a-162e7f5d1bc7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1283e-94d6-416a-bc5f-9dec1a6f0053",
   "metadata": {},
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88185d6c-79f1-4891-b984-ed9ac75cebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f17075-0085-41b9-b50a-d28b53fca55e",
   "metadata": {},
   "source": [
    "### Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ffdea0-d1d1-451a-a7e4-114dc8c76b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-3-8b\", \"2.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da5588-a8f8-45a1-ae21-2fcb03f07e42",
   "metadata": {},
   "source": [
    "## Deploy Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c94b22-1ad3-41c9-bc1f-6dad6e2518b9",
   "metadata": {},
   "source": [
    "### Initialize and Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa9e15-2ece-44c1-819d-553128f701d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "# Please change the following line to have accept_eula = True\n",
    "pretrained_predictor = pretrained_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afee19-70da-479e-b28b-aab013926b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8ba25-c24e-404c-b15d-cfec95907dbd",
   "metadata": {},
   "source": [
    "### Test Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08edd453-6f0f-45ac-9bc0-c205eab588eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response.get('generated_text')}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463a8e3-bc61-4049-94d4-a1e4bb95b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=false\"\n",
    "    )\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed06f74-7f4f-44c9-bfa1-25d9eaa48966",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b394019-fac5-4852-b794-52bac71f5841",
   "metadata": {},
   "source": [
    "### Load and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d23a1-4897-4cb9-a9a3-43cce32983c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(\n",
    "    lambda example: example[\"category\"] == \"summarization\"\n",
    ")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72f2d9-3e98-43c1-9aeb-84912e02ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25b002-8a1b-41f4-aab0-9bd2ad17be6e",
   "metadata": {},
   "source": [
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8955c6-b947-41f8-8eae-c6a1f2dcf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66044b-2e48-409d-a953-aa4fb2101569",
   "metadata": {},
   "source": [
    "### Upload Training Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8fe19-8f5a-44b5-9d45-5d72e60ec6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71505504-ea9c-4be7-8b0d-ae7d9f3767a6",
   "metadata": {},
   "source": [
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa97fb7-019b-4bf1-9f72-1115fb9c3c50",
   "metadata": {},
   "source": [
    "### Initialize and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee1347-464f-49e7-9536-4381c69e9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # Please change {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.24xlarge\",  # For Llama-3-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(\n",
    "    instruction_tuned=\"True\", epoch=\"3\", max_input_length=\"1024\"\n",
    ")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f1bbf-cfcb-487e-933c-4d52ac7dd9ef",
   "metadata": {},
   "source": [
    "### Deploy Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b90c8-edd3-4be4-a844-dbb07a7cfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140c4cd-8ec0-4a71-82c2-3ea8ae258f33",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e65da-e043-46ec-b5e9-9a2494a18797",
   "metadata": {},
   "source": [
    "### Compare Original and Fine-tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0a3ef-69ad-4605-85ee-044829e7a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "(\n",
    "    inputs,\n",
    "    ground_truth_responses,\n",
    "    responses_before_finetuning,\n",
    "    responses_after_finetuning,\n",
    ") = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=true\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=false\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response.get(\"generated_text\"))\n",
    "    # Fine Tuned Llama 3 models doesn't required to set \"accept_eula=true\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    responses_after_finetuning.append(finetuned_response.get(\"generated_text\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ecee2-1940-4e85-ab89-f4b970223584",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully fine-tuned Llama 3 8B on summarization tasks using SageMaker JumpStart. The process involved:\n",
    "\n",
    "1. Deploying a pre-trained Llama 3 model as a baseline\n",
    "2. Preparing the Dolly dataset focused on summarization tasks\n",
    "3. Creating appropriate prompt templates for instruction tuning\n",
    "4. Configuring and executing the fine-tuning job\n",
    "5. Deploying the fine-tuned model as an endpoint\n",
    "6. Comparing the performance between the original and fine-tuned models\n",
    "\n",
    "The results demonstrate how fine-tuning can significantly improve the model's summarization capabilities, producing more concise and accurate summaries tailored to the specific style and format of our training data.\n",
    "\n",
    "This approach can be extended to other tasks like question answering, information extraction, or creative writing by selecting the appropriate subset of the Dolly dataset and adjusting the prompt template accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fa082-1c29-4044-9921-179d08ba95bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
