{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b6c97f-ece1-4bfe-9ec5-c737b25299ca",
   "metadata": {},
   "source": [
    "# Chapter 11 - RAG and Model Evaluation: Evaluating Amazon Bedrock Knowledge Bases with RAGAS Framework\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build and evaluate a Question & Answer application using Amazon Bedrock Knowledge Bases with the Retrieval Augmented Generation Assessment (RAGAS) framework. We'll use Amazon Bedrock's Retrieve API to perform semantic search and evaluate responses using RAGAS evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f10cf7-b7e5-4ced-9bdd-a7402fa936ad",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to build and evaluate a Question & Answer application using Amazon Bedrock Knowledge Bases with the Retrieval Augmented Generation Assessment (RAGAS) framework. We'll use Amazon Bedrock's Retrieve API to perform semantic search against a knowledge base, generate responses with Anthropic Claude, and evaluate those responses using the RAGAS evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e74a83-bd30-4ac9-9e66-c9214435da38",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- An Amazon Bedrock Knowledge Base created and populated with documents\n",
    "- Knowledge Base ID available from a previous setup step\n",
    "- Access to Amazon Bedrock foundation models (Claude 3 Haiku and Claude 3 Sonnet)\n",
    "- Python 3.10 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96f768-5524-4fde-8927-f4fb38a741b8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f400d6b-7aff-44dd-ac9c-1624ff62e014",
   "metadata": {},
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb949ccc-c388-464a-b4e2-437600fda6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip --quiet\n",
    "%pip install -r requirements.txt --no-deps --quiet\n",
    "%pip install -r requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e7cf6-c01c-49ca-b35c-400bd97f3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b336293-47c2-49df-92ad-1da126861d67",
   "metadata": {},
   "source": [
    "### Initialize AWS Clients and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5004443-4492-4df0-a5cf-4bf204a77c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id = \"SI12QCDPJO\" # Replace with your knowledge base id here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33aa02-4cb1-4e67-8d0b-d33fa4ecf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "# Configure Bedrock clients with appropriate timeouts\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config\n",
    "                              )\n",
    "# Initialize Claude 3 Haiku for text generation\n",
    "llm_for_text_generation = BedrockChat(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\", client=bedrock_client)\n",
    "# Initialize Claude 3 Sonnet for evaluation (more powerful model)\n",
    "llm_for_evaluation = BedrockChat(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", client=bedrock_client)\n",
    "# Initialize Titan embeddings model\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\",client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6f6be-7c69-4e50-bc57-72a185325591",
   "metadata": {},
   "source": [
    "### Create Retriever from Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f2f36-4004-4954-b719-4b1812c887e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Amazon Knowledge Bases retriever to fetch top 5 results\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "        knowledge_base_id=kb_id,\n",
    "        retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 5}},\n",
    "        # endpoint_url=endpoint_url,\n",
    "        # region_name=\"us-east-1\",\n",
    "        # credentials_profile_name=\"<profile_name>\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca89ea-086d-435e-a2c3-eac424962174",
   "metadata": {},
   "source": [
    "## Generate Model Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc889803-1835-478c-aa9e-c77297b91b19",
   "metadata": {},
   "source": [
    "### Set Up Retrieval QA Chain and Test with a Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8c777-f72d-428e-978c-13d4771fdd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did Amazon's operating income change in 2023 compared to 2022?\"\n",
    "# Create a RetrievalQA chain that combines retrieval and LLM generation\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_for_text_generation, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "# Generate a response to the query\n",
    "response = qa_chain.invoke(query)\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c19649-f434-44a5-818a-f62c57282a3a",
   "metadata": {},
   "source": [
    "## Prepare Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7eae62-cd93-4cae-a3a9-eadbe8119fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Define evaluation questions and ground truth answers\n",
    "questions = [\n",
    "    \"How did Amazon's operating income change in 2023 compared to 2022?\",\n",
    "    \"What were the key factors driving Amazon's revenue growth in 2023?\",\n",
    "    \"What is the primary revenue mix for Amazon's AWS segment?\",\n",
    "    \"How does Amazon describe its approach to primitives in its business strategy?\"\n",
    "]\n",
    "ground_truths = [\n",
    "    \"Operating income increased from $12.2 billion in 2022 to $36.9 billion in 2023, which represents a 201% improvement.\",\n",
    "    \"Key factors driving revenue growth included increased unit sales primarily by third-party sellers, advertising sales, and subscription services, as well as increased customer usage in AWS.\",\n",
    "    \"AWS sales primarily come from global sales of compute, storage, database, and other services, with revenue recognized when customers use these services based on quantity of services rendered.\",\n",
    "    \"Amazon describes primitives as discrete, foundational building blocks that builders can weave together. They enable innovation and experimentation at high rates, allowing Amazon to rapidly improve customer experiences.\"\n",
    "]\n",
    "# Generate answers and retrieve contexts for each question\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for query in questions:\n",
    "  answers.append(qa_chain.invoke(query)[\"result\"])\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "# Create dictionary and convert to dataset\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6097c2-7b88-44bb-8cfd-807eb602ef06",
   "metadata": {},
   "source": [
    "## Evaluate with RAGAS Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb83c3-d48a-4747-94e7-b2dfd7ded98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import (\n",
    "harmfulness, \n",
    "maliciousness, \n",
    "coherence, \n",
    "correctness, \n",
    "conciseness\n",
    ")\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy\n",
    "    ]\n",
    "# Run the evaluation\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=metrics,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "# Convert results to pandas DataFrame\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07edfd8-a751-486a-9b45-07d406bb46c5",
   "metadata": {},
   "source": [
    "## Display Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dc614-0035-4211-84ee-76c43c9b2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 800\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e1ddd-a33d-4c6f-b62c-822498f5eaa6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The RAGAS framework provides several key metrics for evaluating RAG systems:\n",
    "\n",
    "- **Faithfulness**: Measures factual consistency between the answer and retrieved context (0-1, higher is better)\n",
    "- **Answer Relevancy**: Assesses how pertinent the answer is to the given query\n",
    "- **Context Precision**: Evaluates if relevant context items are ranked higher\n",
    "- **Context Recall**: Measures how well retrieved context aligns with ground truth\n",
    "- **Context Entity Recall**: Evaluates if entities from ground truth appear in retrieved context\n",
    "- **Answer Semantic Similarity**: Measures semantic resemblance between answer and ground truth\n",
    "- **Answer Correctness**: Gauges accuracy of the answer compared to ground truth\n",
    "- **Aspect Critique**: Assesses submissions on predefined aspects like harmlessness and correctness\n",
    "\n",
    "Note: Based on evaluation results, you may need to optimize your RAG workflow by reviewing your chunking strategy, prompt instructions, or adjusting the number of retrieved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2a462-c280-4d91-85cf-b220a3fac380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
