{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5ea2de-972c-4b88-8c8c-af7c84c9b9b2",
   "metadata": {},
   "source": [
    "# Chapter 7 - Open-source Frameworks: Document Summarization with Amazon Bedrock and LangChain\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build a document summarization system using LangChain integrated with Amazon Bedrock. We'll explore how to process large documents, extract key information, and generate concise summaries using foundation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dee870-2d0f-4d61-8eca-b52247fa1378",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to build a document summarization pipeline using Amazon Bedrock's foundation models and LangChain. We'll process text documents, tokenize them appropriately, and leverage Claude 3 Sonnet to generate concise, accurate summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7217845-93bf-4e9f-9549-524f2cc32a80",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- AWS account with Amazon Bedrock access\n",
    "- Access to Claude 3 Sonnet model\n",
    "- Text documents for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89cf69-48d4-43b7-a3fd-553e2a0aae9a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15d651-c30b-4714-a70b-21a35ed4182d",
   "metadata": {},
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a245cf-857e-40f3-9104-d78aa6addc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing boto3 package with pip, using upgrade flag and disabling cache\n",
    "%pip install -U --no-cache-dir boto3\n",
    "%pip install -U --no-cache-dir  \\\n",
    "    \"langchain>=0.1.11\" \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0\n",
    "%pip install -U --no-cache-dir transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0696978-2da9-4b07-99e2-eeddc399104e",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083aa877-5be0-4621-8d6a-e15adcc40444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Python modules\n",
    "import warnings  \n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "import json\n",
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003397b4-afe5-43b5-a3ed-128d140c6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a client for Amazon Bedrock runtime service\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff7c56-3372-4e8e-88e0-0aad9afa59b0",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa5355-6506-4d09-893a-20b459a1d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing warning messages to keep output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "# Defining a utility function to print text with word wrapping at specified width\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833de13-9c3c-4947-93c6-30ac1c3d00f4",
   "metadata": {},
   "source": [
    "## Document Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df1a96-4367-4b21-abd4-568aef80c7b8",
   "metadata": {},
   "source": [
    "### Load and Split Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493f68-124b-4429-a1f5-0b7b17f15763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LangChain modules for document processing and chain creation\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a4ee1-97cd-4c78-a8e2-a20a9b093d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing PyPDF2 library for PDF processing\n",
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34398bce-dcb1-41f2-869d-0d0229b10b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "loader = TextLoader('data/noob.txt')\n",
    "data = loader.load()\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05204b2-4494-48ba-9b18-9d7cf75d1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52f0c6-ba96-49c6-bac2-4dc0e53d2afd",
   "metadata": {},
   "source": [
    "### Analyze Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e7368-567e-4689-a840-96e146017df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting and Displaying the total token count\n",
    "import tiktoken\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Count the total number of tokens\n",
    "total_tokens = 0\n",
    "\n",
    "for text in texts:\n",
    "    tokens = tokenizer.encode(text.page_content)\n",
    "    num_tokens = len(tokens)\n",
    "    total_tokens += num_tokens\n",
    "#    print(f\"Number of tokens in chunk: {num_tokens}\")\n",
    "\n",
    "print(f\"Total number of tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e17d5-dc25-4b34-b927-a2a822f62243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the langchain-aws package quietly (suppressing output)\n",
    "!pip install -U langchain-aws --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47dfb5e-281c-48d5-b261-c60a1fe53a11",
   "metadata": {},
   "source": [
    "## Create Summarization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047024e8-776d-44cf-83bb-e6d784f63974",
   "metadata": {},
   "source": [
    "### Define Summarization Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d027954-3ad2-4ea9-b727-35ffda80c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PromptTemplate to create structured prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Please summarize the following text: {text}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cbdae-f96e-42f7-8b4c-d9b54104be16",
   "metadata": {},
   "source": [
    "### Initialize Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686955e2-8cd3-4eae-92e3-920a604622c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the ChatBedrock class from langchain_aws for Claude integration\n",
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885aaaf7-d149-4a19-8ebe-c8baa48cc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a language model instance using Claude 3 Sonnet\n",
    "llm = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee54e78-6d33-471a-a45b-27988e8e9706",
   "metadata": {},
   "source": [
    "### Build Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba662535-12f0-4950-b374-610d1c2cd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summarize chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b61d6-dca5-4c26-b40c-f750aa6b07a9",
   "metadata": {},
   "source": [
    "## Generate Document Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280f93e-7612-4311-9476-5a629a039a57",
   "metadata": {},
   "source": [
    "### Run Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371567a-d518-4673-8257-bc2adc6b761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the text\n",
    "summary = chain.invoke(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffa604-26be-4224-95a0-4d882f2a40b4",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11e22f-6708-4600-8762-bf3526de8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce493537-1d70-4dd3-84c9-db990489a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print just the output_text\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e582c-4f21-43da-8e94-03bde0b4a996",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've successfully built and demonstrated a document summarization pipeline that leverages Amazon Bedrock's Claude 3 Sonnet model and LangChain's orchestration capabilities. This implementation showcases how modern AI technologies can effectively distill lengthy documents into concise, meaningful summaries while maintaining the core message and key points.\n",
    "\n",
    "Our approach addressed several critical challenges in document summarization:\n",
    "\n",
    "1. **Large Document Processing**: By breaking text into manageable chunks with appropriate overlap, we ensured that even lengthy documents could be processed efficiently without exceeding token limitations.\n",
    "\n",
    "2. **Context Preservation**: The map-reduce summarization strategy allowed us to maintain important context across sections while still generating a cohesive final summary.\n",
    "\n",
    "3. **Token Management**: Using token counting helped us optimize our text splitting strategy, ensuring efficient use of the model's capacity.\n",
    "\n",
    "4. **Foundation Model Integration**: Amazon Bedrock provided a robust, high-quality model that could understand complex text and generate natural summaries without requiring specialized model training.\n",
    "\n",
    "The resulting summarization capability has numerous practical applications, from creating executive briefs of technical documents to processing academic papers, news articles, or legal texts. The approach is flexible enough to be adapted for different document types and summarization requirements by adjusting prompt templates, chunk sizes, or summarization strategies.\n",
    "\n",
    "For future enhancements, consider implementing:\n",
    "- Multi-document summarization for comparative analysis\n",
    "- Custom prompts for different summary styles (extractive vs. abstractive)\n",
    "- Domain-specific summarization by fine-tuning prompts for legal, medical, or technical content\n",
    "- Integration with document management systems for automated summary generation\n",
    "\n",
    "This powerful combination of Amazon Bedrock's foundation models with LangChain's flexible orchestration capabilities demonstrates how enterprises can quickly build practical, production-ready AI solutions that deliver real value by making information more accessible and actionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5711c-a847-467f-bb9a-ec95b89cad65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
