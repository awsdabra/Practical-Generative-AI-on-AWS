{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba126654-7d37-4526-aa56-3e282f10c244",
   "metadata": {},
   "source": [
    "# Chapter 7 - Open-source Frameworks: Building Conversational AI with Amazon Bedrock\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build conversational AI applications using open-source frameworks integrated with Amazon Bedrock. We'll explore how to leverage frameworks like LangChain and LlamaIndex for building sophisticated AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59faf137-cb12-4688-ba4d-ef56ea70385a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to build conversational AI applications using Amazon Bedrock's foundation models. We'll explore two distinct approaches:\n",
    "1. A basic chatbot without context memory\n",
    "2. A context-aware chatbot using Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1503df-b779-4368-8184-9a1677f09fbb",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "- AWS account with Amazon Bedrock access\n",
    "- Access to Amazon Titan and Nova foundation models\n",
    "- PDF document for knowledge extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e270a6-86bd-4243-829f-634e5fb703a4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f79d39-fc03-4c8e-a053-907a97e7bd83",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307c855-4fb5-4e26-9f16-77a9fd610e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade langchain langchain_aws langchain_community \"faiss-cpu>=1.7,<2\" pypdf boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9053da5-b61f-4c61-be6f-ece4d4bc8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264bf840-b509-48a0-bc6d-b18036f8914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Bedrock client\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fdc58-e57f-4555-9abf-fc30468273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for text wrapping\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee8177-d74a-4c47-938f-f33cbb33c11d",
   "metadata": {},
   "source": [
    "## Basic Chatbot Without Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482baa7-fd70-4208-8505-61ca29c0595d",
   "metadata": {},
   "source": [
    "### Setup Conversational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c90f1-8d64-4eee-aee7-fa4866210cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfd6e7-2b77-4c93-b808-d141732f2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM model\n",
    "modelId = \"us.amazon.nova-lite-v1:0\"\n",
    "nova_llm = ChatBedrock(\n",
    "    model_id=modelId, \n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={'temperature': 0.5, 'max_tokens': 700}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8a79c-25cb-4b96-9efe-d384a6321189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T17:33:15.831638Z",
     "iopub.status.busy": "2025-07-08T17:33:15.831106Z",
     "iopub.status.idle": "2025-07-08T17:33:15.836265Z",
     "shell.execute_reply": "2025-07-08T17:33:15.835429Z",
     "shell.execute_reply.started": "2025-07-08T17:33:15.831607Z"
    }
   },
   "source": [
    "### Create Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90014081-23fe-43d6-9708-2711c1aaee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation chain using the modern approach\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "System: The following is a friendly conversation between a knowledgeable helpful assistant and a customer. \n",
    "The assistant is talkative and provides lots of specific details from its context.\n",
    "\n",
    "{history}\n",
    "Human: {input}\n",
    "Assistant:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15048d-e9e1-4faf-bc1d-06e20aa6f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "chain = prompt | nova_llm\n",
    "\n",
    "# Setup for conversation history\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversation_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdd789-94c5-49a8-9024-23086ea2ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the chatbot\n",
    "def chat(input_text, session_id=\"default\"):\n",
    "    response = conversation_with_history.invoke(\n",
    "        {\"input\": input_text},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61ed6a-78dd-4a6e-9031-9611ed86d3eb",
   "metadata": {},
   "source": [
    "### Test Basic Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bcce9-f745-4bdf-8dee-4259668eb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the basic chatbot\n",
    "try:\n",
    "    response = chat(\"Hi there!\")\n",
    "    print_ww(response)\n",
    "except ValueError as error:\n",
    "    if \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42374d9e-becb-4870-b84a-2d36d8202047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another conversation turn\n",
    "print_ww(chat(\"I need a plan to learn generative AI in 30 days\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3dd3d-3f5e-42e1-b72a-5e48a03b67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for course suggestions\n",
    "print_ww(chat(\"Great, can you help in suggesting some courses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafeaf81-b7b5-4edd-8442-662d39269a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End conversation\n",
    "print_ww(chat(\"Thank you for the inputs. Will get back if I need some help\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eaf98d-59c1-4998-b353-771cdd52e8b8",
   "metadata": {},
   "source": [
    "## Context-Aware Chatbot with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8df2d-375b-4616-9b5d-5b0afc1b24b4",
   "metadata": {},
   "source": [
    "### Setup Text Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0daa438-efa9-40c9-9146-e970bb23eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c42ec-a385-46a4-9eba-397c27b23cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Titan embedding model\n",
    "br_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", \n",
    "    client=boto3_bedrock\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8414dcc-608c-48dd-9cff-dd3b385c64cc",
   "metadata": {},
   "source": [
    "### Process Document and Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709c4f3-5adc-4762-ac41-9897ff7be006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF file\n",
    "# data\n",
    "# Note: Ensure the PDF file exists in your environment or adjust the path accordingly\n",
    "pdf_loader = PyPDFLoader(\"data/generative-ai-guide-1.pdf\")\n",
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ac274-05fe-418b-b22b-2b1cb6f0d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the PDF into chunks using the more modern recursive splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "data = text_splitter.split_documents(documents)\n",
    "print(f\"Documents: after split and chunking size={len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d38487-9ca5-44a2-8404-c10184708f6e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001ecfd-d05b-47af-af42-08232e65e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store from documents\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    documents=data,\n",
    "    embedding=br_embeddings\n",
    ")\n",
    "print(f\"vectorstore_faiss: created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711ccfb-dc1b-408c-9307-23a5196343bb",
   "metadata": {},
   "source": [
    "### Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c859391-515b-44b2-816f-313024df9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore_faiss.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65136c-9fa6-424c-98cb-8f52287a8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for RAG\n",
    "rag_prompt_template = \"\"\"\n",
    "Answer the question based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7d0ac-ca65-4148-a3e5-d24641a817f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG chain using modern approach\n",
    "document_chain = create_stuff_documents_chain(titan_llm, rag_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638eb1f1-03a3-4bc1-a05a-05930339ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the RAG system\n",
    "def ask_rag(question):\n",
    "    result = rag_chain.invoke({\"input\": question})\n",
    "    print_ww(result[\"answer\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e9c95-a476-4a72-949e-14d0c86f4e5c",
   "metadata": {},
   "source": [
    "### Test RAG-Based Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee248012-4421-4f97-a67d-8d66e2c59800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG queries\n",
    "print(\"Question 1: What are the tips provided by adobe stock that can approve the generative AI images for sale?\")\n",
    "ask_rag(\"What are the tips provided by adobe stock that can approve the generative AI images for sale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec16841-87c7-4add-8f9c-678e6c617106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuestion 2: Can you elaborate on how to make money?\")\n",
    "ask_rag(\"Can you elaborate on how do you make money\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6922fa-898e-426f-b246-8d248637b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuestion 3: What happens if the image has a person?\")\n",
    "ask_rag(\"What happens if the image has a person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c9e79-fcc2-406e-bcb1-4010e51df8b9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we explored two powerful approaches to building conversational AI systems using Amazon Bedrock foundation models. By implementing both a basic conversational agent and a more sophisticated RAG-based knowledge assistant, we've demonstrated the flexibility and capabilities of modern AI systems for different use cases.\n",
    "\n",
    "The basic chatbot showcased how Amazon Nova can maintain conversational context and generate coherent, helpful responses across multiple turns of dialogue. This approach works well for general-purpose interactions where broad knowledge and conversational fluency are the primary requirements.\n",
    "\n",
    "Our RAG-enhanced chatbot built with Amazon Titan took capabilities further by grounding responses in specific document knowledge. This approach demonstrated how to create AI assistants that can provide accurate, relevant information from particular sources - a crucial capability for enterprise applications where factual precision matters.\n",
    "\n",
    "Key takeaways from this exercise include:\n",
    "\n",
    "1. **Model Selection Matters**: Different models have different strengths - Nova for conversational fluency, Titan for knowledge tasks.\n",
    "\n",
    "2. **Context Management**: Whether through conversation history or document retrieval, managing context is essential for coherent interactions.\n",
    "\n",
    "3. **Vector Embeddings**: Semantic search using embeddings provides a powerful way to match user questions with relevant information.\n",
    "\n",
    "4. **Prompt Engineering**: The structure and content of prompts significantly influence the quality and relevance of AI responses.\n",
    "\n",
    "5. **Integration Power**: Combining foundation models with retrieval systems creates AI applications that are both flexible and reliable.\n",
    "\n",
    "Amazon Bedrock provides a robust foundation for building these applications, offering both the model variety and the operational infrastructure needed for production deployments. As foundation models continue to evolve, the techniques demonstrated in this notebook will become increasingly valuable for creating AI systems that can understand, reason, and communicate effectively across a wide range of domains and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef66df9-1cbe-42ba-95d6-872fbfa887e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
