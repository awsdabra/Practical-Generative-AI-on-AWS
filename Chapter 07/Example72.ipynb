{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b56536-b3ea-4b12-9ce9-349bbfde9d6c",
   "metadata": {},
   "source": [
    "# Chapter 7 - Open-source Frameworks: RAG Pipeline with LlamaIndex\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) pipeline using LlamaIndex integrated with Amazon Bedrock. We'll explore how to create document indexes, perform semantic search, and generate contextually relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a38a15-950d-41ad-a909-98479f258e55",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to build a basic Retrieval-Augmented Generation (RAG) pipeline using LlamaIndex. The pipeline leverages Amazon Bedrock's foundational models to create an efficient question-answering system based on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc896c-0259-4e6a-8435-7a1d74ec33d0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- AWS account with Amazon Bedrock access\n",
    "- Permissions to use Claude 3 Sonnet and Titan embedding models\n",
    "- PDF document for knowledge extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aace4a0-ad7a-4caf-ac49-090bed17e4ec",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a55d0b-08e1-471c-9c17-9d2ba77d4fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T17:25:42.108498Z",
     "iopub.status.busy": "2025-07-08T17:25:42.107777Z",
     "iopub.status.idle": "2025-07-08T17:25:42.112736Z",
     "shell.execute_reply": "2025-07-08T17:25:42.111859Z",
     "shell.execute_reply.started": "2025-07-08T17:25:42.108463Z"
    }
   },
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571eba28-1323-481b-9ea7-1c66f019e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index --quiet\n",
    "%pip install llama-index-llms-bedrock --quiet\n",
    "%pip install llama-index-embeddings-bedrock --quiet\n",
    "%pip install llama-index-embeddings-huggingface --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f150b1-3372-44b7-952d-5a47cc183a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6300d8-9965-4940-953e-27551cd94b38",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb8b3f-7fa5-438d-964e-d0dfc28f44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pydantic --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb4ade-7198-497d-a764-725b8cad6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-readers-file --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a56613-44f9-471d-a079-aad8d0499669",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-vector-stores-faiss --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfb8ff-b692-41d8-85ef-6a7a109b0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-llms-bedrock-converse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00cf0df-b8fc-4daf-b4ad-344521d167ed",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1348ed-79c0-4cfb-a076-8efce8a269d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse  # Updated import\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd2944-f2ae-42e0-aac0-01042f1b13c5",
   "metadata": {},
   "source": [
    "## Configure Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a1ef84-03d2-4958-91d1-8206c2a35419",
   "metadata": {},
   "source": [
    "### Setup LLM and Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5223d13-8f5e-47f2-839d-44306f771d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude 3 Sonnet as our LLM\n",
    "llm = BedrockConverse(model=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "embedding = BedrockEmbedding(model=\"amazon.titan-embed-text-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe7354-07d9-47b4-9256-9f6f1cc0b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import Settings\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360630c4-8908-461c-b8f3-a516eaba2bf7",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031480ee-60ba-45a7-90ca-4b04b6c98ea6",
   "metadata": {},
   "source": [
    "### Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4c6ca-e894-46f8-8f28-5f1d253da211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a PDF file\n",
    "documents = PDFReader().load_data(file='data/generative-ai-report.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce463e-65a0-4667-b2f4-ae43716a92dd",
   "metadata": {},
   "source": [
    "## Create Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d6f2d-799f-4dfa-86aa-51d7697f8713",
   "metadata": {},
   "source": [
    "### Initialize FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdab6c-745a-4abb-b8e7-7cc472645301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# dimensions of titan text embedding\n",
    "d = 1024\n",
    "faiss_index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed213d-046a-4d32-ac50-dfc8c70bdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store index using FAISS\n",
    "vector_store = FaissVectorStore(faiss_index = faiss_index)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    llm=llm,\n",
    "    embedding=embedding,\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb80d1c-fa9c-4090-96fb-8abc106a8ffe",
   "metadata": {},
   "source": [
    "## Query the RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c6ef2-54d7-42d9-baba-4b947c387ac4",
   "metadata": {},
   "source": [
    "### Create Query Engine and Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33dedf-4159-4f2c-95e4-8a4e8b2ee3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Who are the participants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef409237-e8cd-41b5-b301-0bd66143fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df0d6d-4bd9-46f5-aca2-f038099c5111",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"List the large language models mentioned in this document\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd491a2b-c613-40bf-95c7-306698b55192",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've successfully built a Retrieval-Augmented Generation (RAG) pipeline using LlamaIndex and Amazon Bedrock's foundation models. By leveraging Claude 3 Sonnet for generation and Titan Text Embeddings for semantic representation, we created a powerful question-answering system that grounds its responses in specific document knowledge.\n",
    "\n",
    "The workflow we implemented demonstrates the core components of an effective RAG system:\n",
    "1. Document ingestion and chunking\n",
    "2. Vector embedding generation\n",
    "3. Efficient storage using FAISS vector database\n",
    "4. Semantic retrieval of relevant context\n",
    "5. Augmented generation with retrieved information\n",
    "\n",
    "This approach addresses one of the fundamental challenges of working with LLMs: providing factual, relevant answers based on specific knowledge sources rather than general pretrained information. Our implementation shows how RAG can significantly improve the reliability and contextual accuracy of AI-generated responses.\n",
    "\n",
    "The pipeline we built is flexible and can be extended to handle various document types, knowledge domains, and use cases. By adjusting parameters like chunk size, retrieval methods, or prompt engineering techniques, you can further optimize the system for your specific needs.\n",
    "\n",
    "For production deployments, consider implementing additional components such as:\n",
    "- Persistent storage for your vector index\n",
    "- Monitoring for response quality\n",
    "- User feedback mechanisms\n",
    "- Hybrid retrieval approaches combining semantic and keyword search\n",
    "\n",
    "RAG represents a powerful paradigm for building AI applications that combine the flexibility of generative models with the reliability of information retrieval systems, enabling more trustworthy and useful AI assistants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad67c6-44ce-468f-89a8-93e577843c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
