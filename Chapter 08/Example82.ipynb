{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 - Custom Models: Continued Pre-training on Amazon Bedrock\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to perform continued pre-training on Amazon Bedrock foundation models. We'll explore how to extend pre-trained models with domain-specific knowledge and adapt them for specialized use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates how to perform continued pre-training on Amazon Bedrock using custom datasets. Continued pre-training allows you to adapt foundation models to specific domains, industries, or writing styles by exposing them to relevant text data. This approach enhances model performance on domain-specific tasks without requiring full fine-tuning or extensive prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- AWS account with Amazon Bedrock access\n",
    "- Access to Amazon Titan Text models\n",
    "- Custom text dataset for continued pre-training\n",
    "- Appropriate IAM permissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for dataset handling\n",
    "!pip install datasets==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3      # AWS SDK for Python\n",
    "import json       # JSON handling\n",
    "import datetime   # Date/time operations\n",
    "import os         # Operating system interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Resource Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Bucket and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Preparation\n",
    "# Initialize AWS clients\n",
    "iam = boto3.client(\"iam\")\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Get current AWS account ID for unique resource naming\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "bucket_name = f\"bedrock-pretraining1-{account_id}\"\n",
    "\n",
    "# Create S3 bucket for storing training data\n",
    "print(f\"Creating S3 bucket: {bucket_name}\")\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Create IAM role that Bedrock can assume\n",
    "role_name = f\"Bedrock-Pretraining-Role1-{account_id}\"\n",
    "print(f\"Creating IAM role: {role_name}\")\n",
    "\n",
    "# Define trust policy allowing Bedrock to assume this role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "role = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    ")[\"Role\"][\"RoleName\"]\n",
    "\n",
    "# Create IAM policy with S3 permissions\n",
    "policy_name = \"Bedrock-Pretraining-Role1-Policy\"\n",
    "print(f\"Creating IAM policy: {policy_name}\")\n",
    "\n",
    "# Define policy allowing S3 operations on our bucket\n",
    "s3_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",      # Read objects from S3\n",
    "                \"s3:PutObject\",      # Write objects to S3\n",
    "                \"s3:ListBucket\"      # List bucket contents\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{bucket_name}\",     # Bucket itself\n",
    "                f\"arn:aws:s3:::{bucket_name}/*\"    # All objects in bucket\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "policy_arn = iam.create_policy(\n",
    "    PolicyName=policy_name,\n",
    "    PolicyDocument=json.dumps(s3_policy)\n",
    ")[\"Policy\"][\"Arn\"]\n",
    "\n",
    "# Attach the policy to the role\n",
    "iam.attach_role_policy(\n",
    "    RoleName=role,\n",
    "    PolicyArn=policy_arn\n",
    ")\n",
    "\n",
    "print(\"‚úÖ AWS resources created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets library for data handling\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the pre-training dataset from JSONL file\n",
    "# This dataset contains the text data we want to continue pre-training on\n",
    "print(\"Loading dataset from JSONL file...\")\n",
    "dataset = load_dataset('json', data_files='data/pretraining_dataset.jsonl', split='train')\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training (90%) and validation (10%) sets\n",
    "# Validation data helps monitor training progress and prevent overfitting\n",
    "print(\"Splitting dataset into train/validation sets...\")\n",
    "train_and_validation_dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "print(f\"Training examples: {len(train_and_validation_dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(train_and_validation_dataset['test'])}\")\n",
    "\n",
    "# Create directory for processed datasets\n",
    "dataset_dir = \"dataset\"\n",
    "\n",
    "def format_save_dataset(filename, dataset):\n",
    "    \"\"\"\n",
    "    Format and save dataset in the format expected by Bedrock.\n",
    "    Each line contains a JSON object with an 'input' field.\n",
    "    \"\"\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{dataset_dir}/{filename}\", \"w\") as f:\n",
    "        for example in dataset:\n",
    "            # Extract content from the 'input' field\n",
    "            content = example[\"input\"]\n",
    "            \n",
    "            # Format as required by Bedrock pre-training\n",
    "            formatted_example = {\n",
    "                \"input\": content\n",
    "            }\n",
    "            \n",
    "            # Write as JSONL (one JSON object per line)\n",
    "            json.dump(formatted_example, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"Saved {len(dataset)} examples to {filename}\")\n",
    "\n",
    "# Save formatted datasets\n",
    "format_save_dataset(\"train.jsonl\", train_and_validation_dataset[\"train\"])\n",
    "format_save_dataset(\"validation.jsonl\", train_and_validation_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload formatted datasets to S3 bucket\n",
    "print(\"Uploading datasets to S3...\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "bucket_name = f\"bedrock-pretraining-{account_id}\"  # Note: Different bucket name\n",
    "\n",
    "# Walk through dataset directory and upload all files\n",
    "uploaded_files = []\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        # Get full local path\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        # Get relative path for S3 key\n",
    "        relative_path = os.path.relpath(full_path, dataset_dir)\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3.upload_file(full_path, bucket_name, relative_path)\n",
    "        uploaded_files.append(relative_path)\n",
    "        print(f\"Uploaded: {relative_path}\")\n",
    "\n",
    "print(f\"‚úÖ Successfully uploaded {len(uploaded_files)} files to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Customization Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pre-training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock client for model customization\n",
    "bedrock = boto3.client(service_name='bedrock')\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training job parameters\n",
    "datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Job configuration\n",
    "customizationType = \"CONTINUED_PRE_TRAINING\"  # Type of customization\n",
    "customModelName = \"pretrained-titan-lite-model\"  # Name for our custom model\n",
    "jobName = f\"Titan-Lite-Pretraining-Job-{datetime_string}\"  # Unique job name\n",
    "\n",
    "# Base model to continue training from\n",
    "baseModelIdentifier = \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1:0:4k\"\n",
    "\n",
    "# IAM role for Bedrock to access S3\n",
    "roleArn = f\"arn:aws:iam::{account_id}:role/Bedrock-Pretraining-Role-{account_id}\"\n",
    "\n",
    "# Training hyperparameters\n",
    "hyperParameters = {\n",
    "    \"epochCount\": \"1\",                    # Number of training epochs\n",
    "    \"batchSize\": \"1\",                    # Batch size for training\n",
    "    \"learningRate\": \".0001\",              # Learning rate\n",
    "    \"learningRateWarmupSteps\": \"0\"       # Warmup steps\n",
    "}\n",
    "\n",
    "print(f\"Creating model customization job: {jobName}\")\n",
    "print(f\"Base model: Amazon Titan Text Lite\")\n",
    "print(f\"Training epochs: {hyperParameters['epochCount']}\")\n",
    "\n",
    "# Create the model customization job\n",
    "response_ft = bedrock.create_model_customization_job(\n",
    "    jobName=jobName,\n",
    "    customModelName=customModelName,\n",
    "    customizationType=customizationType,\n",
    "    roleArn=roleArn,\n",
    "    baseModelIdentifier=baseModelIdentifier,\n",
    "    hyperParameters=hyperParameters,\n",
    "    \n",
    "    # Training data location\n",
    "    trainingDataConfig={\n",
    "        \"s3Uri\": f\"s3://bedrock-pretraining-{account_id}/train.jsonl\"\n",
    "    },\n",
    "    \n",
    "    # Validation data location\n",
    "    validationDataConfig={\n",
    "        'validators': [{\n",
    "            \"s3Uri\": f\"s3://bedrock-pretraining-{account_id}/validation.jsonl\"\n",
    "        }]\n",
    "    },\n",
    "    \n",
    "    # Output location for training artifacts\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": f\"s3://bedrock-pretraining-{account_id}/pretraining-output\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model customization job created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the job ARN for reference\n",
    "jobArn = response_ft.get('jobArn')\n",
    "print(f\"Job ARN: {jobArn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current status of the training job\n",
    "# Run this cell periodically to monitor progress\n",
    "\n",
    "job_details = bedrock.get_model_customization_job(jobIdentifier=jobName)\n",
    "status = job_details[\"status\"]\n",
    "\n",
    "print(f\"Job Status: {status}\")\n",
    "\n",
    "# Display additional job information\n",
    "if 'creationTime' in job_details:\n",
    "    print(f\"Started: {job_details['creationTime']}\")\n",
    "\n",
    "if status == \"Complete\":\n",
    "    print(\"üéâ Training completed successfully!\")\n",
    "elif status == \"InProgress\":\n",
    "    print(\"‚è≥ Training in progress... Please wait and check again later.\")\n",
    "elif status == \"Failed\":\n",
    "    print(\"‚ùå Training failed. Check the job details for error information.\")\n",
    "else:\n",
    "    print(f\"Current status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purchase Provisioned Throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchase provisioned throughput for the custom model\n",
    "# This allocates dedicated compute resources for inference\n",
    "\n",
    "provisioned_model_name = \"ProvisionedCustomTitanLite\"\n",
    "model_units = 1  # Number of model units to provision\n",
    "\n",
    "print(f\"Creating provisioned throughput: {provisioned_model_name}\")\n",
    "print(f\"Model units: {model_units}\")\n",
    "\n",
    "response_pt = bedrock.create_provisioned_model_throughput(\n",
    "    modelId=customModelName,\n",
    "    provisionedModelName=provisioned_model_name,\n",
    "    modelUnits=model_units\n",
    ")\n",
    "\n",
    "# Get the ARN of the provisioned model for inference\n",
    "provisionedModelArn = response_pt.get('provisionedModelArn')\n",
    "print(f\"Provisioned Model ARN: {provisionedModelArn}\")\n",
    "\n",
    "print(\"‚úÖ Provisioned throughput created successfully!\")\n",
    "print(\"üí∞ Note: This will incur ongoing costs until deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock Runtime client for model inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "# Define your test prompt here\n",
    "# Replace with a prompt relevant to your training data\n",
    "prompt = \"<ENTER_PROMPT>\"  # TODO: Replace with your actual prompt\n",
    "\n",
    "# Configure inference parameters\n",
    "inference_config = {\n",
    "    \"prompt\": prompt,\n",
    "    \"temperature\": 0.5,      # Controls randomness (0.0 = deterministic, 1.0 = very random)\n",
    "    \"p\": 0.9,               # Top-p sampling parameter\n",
    "    \"max_tokens\": 512      # Maximum tokens to generate\n",
    "}\n",
    "\n",
    "print(f\"Testing custom model with prompt: {prompt[:50]}...\")\n",
    "\n",
    "# Invoke the custom model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=provisionedModelArn,  # Use our provisioned custom model\n",
    "    body=json.dumps(inference_config)\n",
    ")\n",
    "\n",
    "# Parse and display the response\n",
    "response_body = json.loads(response['body'].read())\n",
    "generated_text = response_body.get('outputText', '')\n",
    "\n",
    "print(\"\\n=== Model Response ===\")\n",
    "print(generated_text)\n",
    "print(\"\\n‚úÖ Custom model inference completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup (Important!)\n",
    "\n",
    "‚ö†Ô∏è **Don't forget to clean up resources** to avoid ongoing charges:\n",
    "\n",
    "1. **Delete provisioned throughput** (incurs hourly costs)\n",
    "2. **Delete custom model** (if no longer needed)\n",
    "3. **Delete S3 bucket** and contents\n",
    "4. **Delete IAM role and policy**\n",
    "\n",
    "```python\n",
    "# Example cleanup commands:\n",
    "bedrock.delete_provisioned_model_throughput(provisionedModelId='ProvisionedCustomTitanLite')\n",
    "bedrock.delete_custom_model(modelIdentifier=customModelName)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the complete workflow for continued pre-training of foundation models on Amazon Bedrock. This approach enables you to adapt large language models to specific domains, industries, or writing styles by exposing them to targeted text data.\n",
    "\n",
    "Key accomplishments:\n",
    "\n",
    "1. **Resource Setup**: We established the necessary AWS infrastructure including secure IAM roles and S3 storage for our training data.\n",
    "\n",
    "2. **Dataset Preparation**: We loaded, processed, and formatted our custom dataset into the structure required by Bedrock's continued pre-training process.\n",
    "\n",
    "3. **Model Customization**: We created a customization job that builds upon the Amazon Titan Text Lite foundation model, adapting it to our specific domain using our custom dataset.\n",
    "\n",
    "4. **Deployment**: We provisioned throughput resources to make our custom model available for inference.\n",
    "\n",
    "5. **Model Testing**: We demonstrated how to use the custom pre-trained model to generate text that reflects the characteristics of our training data.\n",
    "\n",
    "Continued pre-training offers several advantages over other customization approaches:\n",
    "\n",
    "- **Domain Adaptation**: Models become more fluent and knowledgeable about specific domains or industries\n",
    "- **Style Adaptation**: The model can better match the writing style, tone, and terminology of your organization\n",
    "- **Knowledge Integration**: The model implicitly learns information embedded in your training texts\n",
    "- **Efficiency**: Often more efficient than fine-tuning for general domain adaptation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
